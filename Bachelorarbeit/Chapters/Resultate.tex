\section{Resultate}
In der nachfolgenden Analyse liegt der Fokus auf der Evaluierung der implementierten Varianten des Shor-Algorithmus hinsichtlich ihres Ressourcenverbrauchs und 
der Laufzeitkomplexität. 
Angesichts der signifikanten Auswirkungen des Algorithmus für die Integrität des kryptografischen Systems des RSA-Verfahrens  
orientieren sich die zugrunde gelegten Bewertungskriterien an den Richtlinien für Post-Quantenkryptografie des \textit{National Institute of Standards and Technology}(NIST). 
Die erzielten Resultate ermöglichen eine Einordnung in die Größenordnung der erforderlichen Laufzeit und 
erlauben einen direkten Vergleich mit den Ergebnissen anderer Arbeiten.

\subsection*{Laufzeit}
In der Publikation "'Submission Requirements and Evaluation Criteria for the Post-Quantum Cryptography Standardization Process"` des NIST~\cite[17]{NISTPQC} 
werden drei Sicherheitsstufen definiert, die sich auf die Laufzeitanforderungen von Quantencomputern in Bezug auf kryptografische Angriffe beziehen.

Als Metrik für die Laufzeitevaluation wird der Parameter \textit{Maxdepth} eingeführt, 
welcher die Anzahl der Quantengatter für die längste serielle Ausführung eines Quantenalgorithmus erfasst.
Die Auswahl dieser Metrik wird in der Publikation damit begründet, dass lange serielle Berechnungen Herausforderungen mit sich bringen.

Die schwächste der drei Sicherheitsstufen beginnt bei einer \textit{Maxdepth} von \(2^{40}\) Quantengattern. 
Dieser Wert dient als approximative Messgröße für die Anzahl an seriell ausgeführten Quantengattern, 
die nach aktueller Prognose von Quantencomputern innerhalb eines Jahres realisiert werden können.

Die nächste Sicherheitsstufe setzt eine \textit{Maxdepth} von \(2^{64}\) Quantengattern an.
Dieser Wert stellt die approximative Anzahl an seriell ausgeführten Quantengattern dar, 
die nach der Prognose innerhalb eines Jahrzehnts durchgeführt werden können.

Die stärkste Sicherheitsstufe ist bei einer \textit{Maxdepth} von \(2^{96}\) Quantengattern festgelegt. 
Selbst unter idealen Bedingungen, 
bei denen Qubits auf atomarer Skala arbeiten und die Übertragungsgeschwindigkeit der Lichtgeschwindigkeit entspricht, 
würde diese Anzahl an Quantengattern ein Jahrtausend an Berechnungszeit erfordern.


\subsection*{Ressourcenbedarf}
Als Metriken für den Ressourcenverbrauch werden die benötigte Anzahl an Qubits sowie die Gesamtanzahl an Quantengattern herangezogen. 
Die Anzahl der Qubits stellt eine notwendige Voraussetzung dar, 
die ein Quantencomputer erfüllen muss, da andernfalls die Ausführung des Quantenalgorithmus nicht möglich ist. 
Die Gesamtanzahl an Quantengattern dient als eine Kennzahl, 
die potenzielle Rückschlüsse auf die Fehlerrate durch Dekohärenz zulässt. 
Darüber hinaus finden sowohl die Anzahl an Qubits als auch die Gesamtanzahl an Quantengattern häufig Anwendung als Kennzahl in der wissenschaftlichen Literatur. 
Diese beiden Metriken ermöglichen somit einen Vergleich mit den Ergebnissen anderer Arbeiten.

In der Analyse wurden ausschließlich elementare Quantengatter wie Hadamard-, Phasen- und X-Gatter in die Betrachtung einbezogen. 
Die in Sektion~\ref{sec:Implementierung} eingeführten, spezialisierten Gatter wurden nicht direkt gezählt, 
stattdessen wurden die darin enthaltenen Basisgatter gezählt. 
Dieses Vorgehen ergibt sich aus der Tatsache, dass ein Quantencomputer effektiv die Basisgatter eines Quantenschaltkreises ausführt. 
Übergeordnete, spezialisierte Gatter werden demzufolge in ihre elementaren Komponenten zerlegt. 
Eine Zählung der spezialisierten Gatter wäre somit nicht repräsentativ für die tatsächliche Ressourcennutzung.

Für die Zählung der Anzahl der Gatter sowie der \textit{Maxdepth} des Quantenschaltkreises wurden spezifische Funktionen der Qiskit-Bibliothek genutzt. 
Diese Funktionen erlauben die Bestimmung der Anzahl an Gatter und der \textit{Maxdepth} in einem Quantenschaltkreis.
Dabei ist zu beachten, dass diese Funktionen keine Unterscheidung zwischen elementaren und spezialisierten Gattern treffen.
Daher konnte die Funktionen nicht direkt zur Analyse des vorliegenden Quantenschaltkreises verwendet werden. 
Als Lösungsansatz wurde der Code zur Zählung modifiziert, um sicherzustellen, dass der Quantenschaltkreis ausschließlich aus Basisgattern besteht.

\subsection*{Analyse}
Die Analyse beinhaltet eine Untersuchung von vier Varianten des Shor-Algorithmus, 
die jeweils unterschiedliche Kombinationen der in Sektion~\ref{Optimierung} erörterten Optimierungsstrategien implementieren.
Die vier Varianten werden nach den implementierten Optimierungsstrategien in die Kategorien \textit{Reguläre}, 
\textit{Approximative}, \textit{Iterative} und \textit{Approximative-Iterative} eingeteilt.
In den Kategorien unterscheidet sich insbesondere die Nutzung der Quantum-Phase-Estimation und der Quanten-Fourier-Transformation. 
Während die \textit{Reguläre} Variante auf die \(4n+2\) Qubit Version der Quantum-Phase-Estimation mit der exakten Quanten-Fourier-Transformation setzt, 
nutzt die \textit{Approximative} Variante ebenfalls die \(4n+2\) Qubit Variante, 
verwendet jedoch die approximative Quanten-Fourier-Transformation mit einem Approximationsfaktor von \(m = \lceil\text{ld}(n)+2\rceil\).

Die \textit{Iterative} Variante bedient sich der iterativen Quantum-Phase-Estimation mit \(2n+3\) Qubits und 
verwendet die exakte Quanten-Fourier-Transformation, dementsprechend in iterativer Anwendung.
Die \textit{Approximative-Iterative} Variante verwendet ebenfalls die iterative Quantum-Phase-Estimation mit \(2n+3\) Qubits, 
aber ergänzt die iterativ angewendete Quanten-Fourier-Transformation um einen Approximationsfaktor von \\\(m = \lceil\text{ld}(n)+2\rceil\).

Die Analyse bezieht sich auf die Schlüssellängen des RSA-Kryptosystems von 1024-Bit, 2048-Bit, 3072-Bit und 4096-Bit. 
Nach aktuellem Stand empfiehlt das BSI für das RSA-Verfahren keine Schlüssellänge unter 3000-Bit~\cite[41]{BSI2023}. 
Nichtsdestotrotz sind heutzutage noch immer Varianten des RSA-Kryptosystems in Einsatz, welche kürzere Schlüssel verwenden, 
weshalb diese ebenfalls berücksichtigt werden.

\vspace{1em}

\begin{table}[H] \label{Varainten_Analyse}
    \centering
    \caption{Vergleich der vier Varianten des Shor-Algorithmus für verschiedene Schlüssellängen}
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        \textbf{Variante} & \textbf{Schlüssellänge} & \textbf{Qubits} & \textbf{Gatteranzahl} & \textbf{Maxdepth} \\ \hline
        Reguläre & 1024-Bit & 4098 & \(2^{43}\) & \(2^{35}\) \\ \hline
        Reguläre & 2048-Bit & 8194 & \(2^{47}\) & \(2^{38}\) \\ \hline
        Reguläre & 3072-Bit & 12290 & \(2^{49}\) & \(2^{40}\) \\ \hline
        Reguläre & 4096-Bit & 16386 & \(2^{51}\) & \(2^{41}\) \\ \hline
        Approximative & 1024-Bit & 4098 & \(2^{38}\) & \(2^{35}\) \\ \hline
        Approximative & 2048-Bit & 8194 & \(2^{41}\) & \(2^{38}\) \\ \hline
        Approximative & 3072-Bit & 12290 & \(2^{43}\) & \(2^{40}\) \\ \hline
        Approximative & 4096-Bit & 16386 & \(2^{44}\) & \(2^{41}\) \\ \hline
        Iterative & 1024-Bit & 2051 & \(2^{43}\) & \(2^{35}\) \\ \hline
        Iterative & 2048-Bit & 4099 & \(2^{47}\) & \(2^{38}\) \\ \hline
        Iterative & 3072-Bit & 6147 & \(2^{49}\) & \(2^{40}\) \\ \hline
        Iterative & 4096-Bit & 8195 & \(2^{51}\) & \(2^{41}\) \\ \hline
        Approximative-Iterative & 1024-Bit & 2051 & \(2^{38}\) & \(2^{35}\) \\ \hline
        Approximative-Iterative & 2048-Bit & 4099 & \(2^{41}\) & \(2^{38}\) \\ \hline
        Approximative-Iterative & 3072-Bit & 6147 & \(2^{43}\) & \(2^{40}\) \\ \hline
        Approximative-Iterative & 4096-Bit & 8195 & \(2^{44}\) & \(2^{41}\) \\ \hline
    \end{tabular}
    \end{table}
    
Anhand der Tabelle~\ref{Varainten_Analyse} fällt auf,
dass die unterschiedlichen Varianten keinen Unterschied in Bezug auf die \textit{Maxdepth} haben.
Stattdessen ist die Schlüssellänge der einzige Faktor, 
der die \textit{Maxdepth} beeinflusst.

Hingegen ermöglicht der Einsatz eines approximativen und iterativen Ansatzes deutliche Einsparungen im Hinblick auf den Ressourcenbedarf. 
Im Vergleich zu einer Schlüssellänge von 4096-Bit ermöglicht die Verwendung der approximativen Quanten-Fourier-Transformation im Vergleich zur exakten Variante eine Reduzierung der Gesamtanzahl an Gattern um etwa den Faktor 100.
Wie in Sektion~\ref{sec:ApproxQFT} erwähnt, ist der Genauigkeitsverlust durch die approximative Variante für große \(N\) vernachlässigbar. 
Des Weiteren schneidet die approximative Variante in Anwesenheit von Dekohärenz aufgrund der geringeren Anzahl an Gattern besser ab~\cite{Barenco_1996}. 
Diese Aspekte sprechen dafür, den approximativen Ansatz grundsätzlich vorzuziehen.

\subsection*{Vergleich}
In Abschnitt~\ref{sec:QuantumAdder} wurde erläutert, 
dass die Verwendung klassischer Volladdierer in einem Quantenschaltkreis mehr Ressourcen erfordert als die Nutzung der Quantum-Addition. 
Da der Baustein zur Addition in Shors Algorithmus häufig vorkommt, 
hat dies dementsprechend einen erheblichen Einfluss auf den Gesamtressourcenbedarf.

Um eine Abwägung beider Methoden durchzuführen, 
werden die Ergebnisse der Analyse mit dem berechneten Ressourcenbedarf aus der Publikation "'Estimation of Shor’s Circuit for 2048-bit Integers
based on Quantum Simulator"`~\cite{cryptoeprint:2023/092} verglichen. 
Diese Publikation untersucht den Ressourcenbedarf für einen ähnlichen Ansatz zur Implementierung des Shor-Algorithmus.
Der entscheidende Unterschied besteht darin, 
dass der in dieser Publikation verwendete Quantenschaltkreis klassische Volladdierer nach der Anleitung von 
\textit{Vedral}, \textit{Barenco} und \textit{Ekert}\cite{Vedral_1996} verwendet.

In Tabelle~\ref{Volladdierer_Analyse} sind die Ergebnisse der Publikation dargestellt:

\begin{table}[H] \label{Volladdierer_Analyse}
    \centering
    \caption{Vergleich Shor-Algorithmus mit Volladdierer~\cite{cryptoeprint:2023/092}}
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        \textbf{Variante} & \textbf{Schlüssellänge} & \textbf{Qubits} & \textbf{Gatteranzahl} & \textbf{Maxdepth} \\ \hline
        Volladdierer & 1024-Bit & 5121 & \(2^{38}\) & \(2^{38}\) \\ \hline
        Volladdierer & 2048-Bit & 10241 & \(2^{41}\) & \(2^{41}\) \\ \hline
    \end{tabular}
\end{table}
In der Publikation werden die Ressourcen nur für Schlüssellängen von jeweils \(1024\) und \(2048\) Bit untersucht. 
Nichtsdestotrotz lässt sich im direkten Vergleich feststellen, 
dass die Verwendung der Quanten-Addition in allen Fällen zu einer geringeren \textit{Maxdepth} führt. 
Darüber hinaus wird der zusätzliche Bedarf an Qubits bei Verwendung von Volladdierer offensichtlich. 
Lediglich im Gesamtbedarf an Quantengattern schneidet die Variante mit Volladdierer besser ab als die Varianten, 
welche die exakte Quanten-Fourier-Transformation nutzen. 
Andererseits weist der approximative Ansatz einen äquivalenten Gesamtbedarf an Quantengattern zur Variante mit Volladdierer auf.

\subsection*{Effekt der Nachberechnung}

Bezüglich der \textit{Maxdepth} erfüllt eine Schlüssellänge von mindestens, 
3072 Bit bereits die Kriterien der geringsten Sicherheitsstufe. 
Es ist jedoch zu beachten, dass diese \textit{Maxdepth} nur für einen einzelnen Durchlauf des Quantenalgorithmus gilt. 
Wenn mehrere Durchläufe benötigt werden, steigt dementsprechend auch die \textit{Maxdepth} der gesamten Berechnung.

Wie in Sektion~\ref{Funktionsweise:klassisch} erklärt, kann es ohne klassische Nachberechnungen bis zu \(2\text{ld}(N)\) Messungen erfordern, 
bis ein Messergebnis die ungekürzte Periode enthält. 
In Bezug auf eine Schlüssellänge von 3072 Bit würde das im Worst-Case-Szenario \(2\text{ld}(2^{3072})\) erfordern, 
also in etwa \(2^{13}\) Durchläufe des Quantenalgorithmus.
Dies ergibt eine gesamte \textit{Maxdepth} aller Durchläufe von \(2^{40} \cdot 2^{13}\), insgesamt also \(2^{53}\).
Unter Berücksichtigung der erforderlichen Laufzeiten wird die Sinnhaftigkeit intensiver Nachberechnungen mit klassischen Methoden verdeutlicht.

Um einen Vergleich zwischen den benötigten Durchläufen mit Nachberechnung und den Durchläufen ohne Nachberechnung zu ermöglichen, 
wurden die Erfolgsraten beider Methoden erhoben.

Dazu wurden beide Methoden verwendet, um die Zahl \(N=391\) jeweils 200-mal zu faktorisieren.
Die Statistik beinhaltet die benötigten Durchläufe des Quantenalgorithmus, die pro Faktorisierung notwendig waren.

Die Berechnung des Quantenalgorithmus wurde unter Verwendung des Simulators von Qiskit durchgeführt. 
Bei jedem Durchlauf wurde ein zufälliges \(a\) gewählt.
Um die Statistik nicht zu verfälschen, wurden Durchläufe, die ein \(a\) verwendeten, 
welches ein Faktor von \(N\) war, nicht gezählt.

Beide Methoden verwendeten die Approximative-Iterative Variante des Quantenalgorithmus mit einem \(k=2n\) und \(m=\lceil\text{ld}(n)+2\rceil\).
Dies entspricht den beiden Standardwerten, 
wie in der Implementierung in Sektion~\ref{sec:Implementierung} verwendet wurde. 
Die Nachberechnung wurde nach dem Verfahren, wie in Sektion~\ref{sec:Faktorisierungsalgorithmus} beschrieben, angewendet. 
Die Anzahl der zu überprüfenden Nachbarn und Vielfachen in der klassischen Nachberechnung betrug ebenfalls den standardmäßigen Wert von \(2^{\text{ld}(\text{ld}(N))}\).

\begin{table}[h] \label{Nachberechnung}
    \centering
    \caption{Anzahl der Durchläufe pro Faktorisierung mit Nachberechnung}
    \begin{tabular}{|c|c|c|c|c|c|c|} 
        \hline
        \textbf{Druckläufe} & 1 & 2 & 3 & 4\\
        \hline
        \textbf{Häufigkeit} & 168 & 27 & 3 & 2 \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[h] \label{ohneNachberechnung}
    \centering
    \caption{Anzahl der Durchläufe pro Faktorisierung ohne Nachberechnung}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
        \hline
        \textbf{Druckläufe} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 11 \\
        \hline
        \textbf{Häufigkeit} & 87 & 51 & 28 & 14 & 9 & 5 & 3 & 2 & 1 \\
        \hline
    \end{tabular}
\end{table}

Im direkten Vergleich der Ergebnisse von Tabelle~\ref{Nachberechnung} zu Tabelle~\ref{ohneNachberechnung} fällt auf, 
dass aufgrund der durchgeführten Nachberechnung häufig nur ein einzelner Durchlauf des Quantenalgorithmus nötig war. 
Hingegen ist bei einem Vorgehen ohne Nachberechnung ein einzelner Durchlauf in weniger als der Hälfte der Fälle ausreichend. 
Des Weiteren war bei der Verwendung einer Nachberechnung nur bei 2.5\% aller Faktorisierungen mehr als zwei Durchläufe nötig. 
Wiederum gab es ohne Nachberechnung in mehr als 30\% der Fälle die Notwendigkeit für mehr als zwei Durchläufe. 
Dazu kommt, 
dass das gemessene Worst-Case-Szenario ohne Nachberechnung um fast Faktor 3 mehr Durchläufe benötigte als bei der Verwendung von Nachberechnungen.

Unter Beachtung der durchschnittlichen Durchläufe pro Faktorisierung beläuft sich der Durchschnittswert mit Nachberechnungen auf \(1,2\). 
Bei einem Vorgehen ohne Nachberechnung sind durchschnittlich \(2,26\) Durchläufe nötig. 
In Bezug auf die \textit{Maxdepth} erhöht das Auslassen einer Nachberechnung diesen Wert um eine Potenz von 2.

Es wurden keine größeren Zahlen zur Auswertung gewählt, 
da die Simulation bereits für diese Größenordnung bei 200 Durchläufen mehrere Tage in Anspruch nahm.

\subsection*{Kritik}

Anhand der in Tabelle~\ref{Varainten_Analyse} aufgeführten \textit{Maxdepth}-Werte lässt sich erkennen, 
dass selbst unter optimalen Bedingungen, bei denen ein einzelner Durchlauf zur Faktorisierung führt, 
der implementierte Quantenalgorithmus für Schlüssellängen ab 3072 Bit eine Laufzeit erfordert, 
die den niedrigsten Sicherheitsanforderungen gemäß den NIST-Kriterien entspricht. 
Dies verdeutlicht, dass die verwendete Implementierung des Quantenschaltkreises in dieser Arbeit keine effiziente Methode darstellt, 
um ein RSA-Verfahren mit den derzeit empfohlenen Schlüssellängen zu kompromittieren.

Stattdessen wird für die effiziente Faktorisierung ein Quantenschaltkreis benötigt, 
der für Schlüssellängen mit mehr als 3072 Bit eine geringere \textit{Maxdepth} als \(2^{40}\) aufweist.

\vspace{1em}

In Abschnitt~\ref{Funktionsweise:klassisch} wurde darauf hingewiesen, 
dass im Grunde genommen ein Wert \(k\) mit \(k > 2\text{ld}(p)\) ausreichend Genauigkeit gewährt.
Ein möglicher Ansatz zur Reduzierung der \textit{Maxdepth} des hier verwendeten Quantenschaltkreises besteht darin, 
auf eine geringere Größenordnung der Periode zu spekulieren, 
um einen Quantenschaltkreis mit einer kleineren \textit{Maxdepth} zu erreichen~\cite{Shor_1997}. 
Zum Beispiel würde ein \(p\) in der Größenordnung von \(2^{1500}\) mit einem entsprechenden \(k = 2\text{ld}(2^{1500})\) dazu führen, 
dass die \textit{Maxdepth} des Quantenschaltkreises von \(2^{40}\) auf \(2^{37}\) reduziert wird. 
Es ist jedoch wichtig zu betonen, dass die Erfolgswahrscheinlichkeit dieses Ansatzes kritisch zu betrachten ist. 
In einem Szenario mit einem Gesamtraum von \(2^{3072}\) ist es äußerst unwahrscheinlich, 
dass eine Periode mit einem kleineren Wert als \(2^{1500}\) vorkommt.
Konkret ist der gesamte Raum um den Faktor \(\frac{2^{3072}}{2^{1500}} = 2^{1572}\) größer als die vermutete Größe der Periode. 
Daher ist die Wahrscheinlichkeit, dass die Periode größer als \(2^{1500}\) ist, 
um einen ähnlichen Faktor höher als die Wahrscheinlichkeit, 
dass sie kleiner als \(2^{1500}\) ist.

\vspace{1em}

Andere Möglichkeiten zur Reduzierung der \textit{Maxdepth} des in dieser Arbeit implementierten Quantenalgorithmus sind nicht bekannt. 
Es liegt nahe anzunehmen, dass für eine effektive Reduktion der \textit{Maxdepth}, ohne signifikante Nachteile, 
eine andere Architektur für einige der verwendeten Bausteine gewählt oder entwickelt werden muss.

\subsection*{Schlussfolgerung}

Daraus lässt sich ableiten, 
dass eine äußerst ressourceneffiziente Implementierung in Bezug auf die Anzahl der benötigten Qubits für die Faktorisierung 
sehr großer \(N\) zu zeitaufwändigen Berechnungen aufgrund einer hohen \textit{Maxdepth} führt. 
Somit ergibt sich die Schlussfolgerung, 
dass der Schwerpunkt bei der Konstruktion des Quantenschaltkreises nicht auf einem möglichst sparsamen Qubit-Bedarf liegen sollte.

Es ist denkbar, dass durch die Verwendung zusätzlicher Qubits ein Quantenschaltkreis mit einer geringeren \textit{Maxdepth} konstruiert werden kann. 
Bei der Konstruktion eines solchen Quantenschaltkreises könnte man sich auf die Parallelisierung der Berechnung konzentrieren. 
Eine weitere potenzielle Einsparmöglichkeit besteht beim Baustein der modularen Addition aus Sektion~\ref{sub:modulareAddition}.
Indem auf die Quantengatter verzichtet wird, 
welche das Bedingungs-Qubit zurücksetzen, 
kann die \textit{Maxdepth} dieses Bausteins um mehr als die Hälfte reduziert werden. 
Anstelle einer Zurücksetzung könnte stattdessen ein neues Qubit verwendet werden. 
Konkrete Ansätze erfordern jedoch eine ausführliche Untersuchung.


